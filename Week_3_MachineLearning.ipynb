{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAT-ExploringAI-2024/Week3-ClassicML/blob/main/Week_3_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywb8NifF9XN3",
        "outputId": "02574e3b-b411-4a94-d624-1abe3bcb3e45"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5CvOjjrL9gw"
      },
      "source": [
        "<center><h1> Introduction to Audio Classification with Machine Learning Models </h1></center>\n",
        "\n",
        "\n",
        "\n",
        "### Purpose\n",
        "This notebook serves as an introduction to working with audio data for classification problems; it is meant as a learning resource rather than a demonstration of the state-of-the-art. The techniques mentioned in this notebook apply not only to classification problems, but to regression problems and problems dealing with other types of input data as well. I provide an introduction to a few key machine learning models and the logic in choosing their hyperparameters. These objectives are framed by the task of recognizing emotion from snippets of speech audio.\n",
        "\n",
        " Training data should be used strictly for training a model, validation data strictly for tuning a model, and test data strictly to evaluate a model once it is tuned - a model should never be tuned to perform better on test data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Classic machine learning models such as Support Vector Machines (SVM), k Nearest Neighbours (kNN), and Random Forests have distinct advantages to deep neural networks in many tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQsTfGREL9g1"
      },
      "source": [
        "<!--TABLE OF CONTENTS-->\n",
        "\n",
        "\n",
        "# Table of Contents\n",
        "  - [Intro: Speech Emotion Recognition on the RAVDESS dataset](#Intro:-Speech-Emotion-Recognition-on-the-RAVDESS-dataset)\n",
        "  - [Machine Learning Process Overview](#Machine-Learning-Process-Overview)\n",
        "  - [Feature Extraction](#Feature-Extraction)\n",
        "    - [Load the Dataset and Compute Features](#Load-the-Dataset-and-Compute-Features)\n",
        "    - [Feature Scaling](#Feature-Scaling)\n",
        "  - [Classical Machine Learning Models](#Classical-Machine-Learning-Models)\n",
        "    - [Training: The 80/20 Split and Validation](#Training:-The-80/20-Split-and-Validation)\n",
        "    - [Comparing Models](#Comparing-Models)\n",
        "    - [The Support Vector Machine Classifier](#The-Support-Vector-Machine-Classifier)\n",
        "    - [k Nearest Neighbours](#k-Nearest-Neighbours)\n",
        "    - [Random Forests](#Random-Forests)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUcbr4PL9g2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Intro: Speech Emotion Recognition on the RAVDESS dataset\n",
        "In this notebook we explore the most common machine learning models, specifically those available off the shelf in scikit-learn.\n",
        "\n",
        "I'm going to use the RAVDESS dataset (Ryerson Audio-Visual Database of Emotional Speech and Song dataset), created by Steven Livingstone and Frank Russo of Ryerson University. <br>\n",
        "[Details of the RAVDESS dataset](https://smartlaboratory.org/ravdess/) <br>\n",
        "[Download the dataset used in this notebook](https://zenodo.org/record/1188976) <br> Scroll half-way down the page and find \"Audio_Speech_Actors_01-24\"<br>\n",
        "\n",
        "We're going to use the audio-only speech portion of the RAVDESS dataset, ~200MB.\n",
        "Audio is sourced from 24 actors (12 male, 12 female) repeating two sentences with\n",
        "a variety of emotions and intensity. We get 1440 speech files (24 actors * 60 recordings per actor). Each audio sample has been rated  by a human 10 times for emotional quality.\n",
        "\n",
        "## Machine Learning Process Overview\n",
        "1. Feature Engineering: Choose and define the properties which our model will use to evaluate the audio files. <br>\n",
        "2. Feature Extraction: Compute the features for each audio file and build a feature matrix representing all audio files. <br>\n",
        "3. Model exploration: Test candidate models that make sense for the properies of the dataset\n",
        "4. Training the MLP Classifier model: Choose and optimize the properties of our model on validation data - hyperparameters and architechture.  <br>\n",
        "5. Evaluate our model's performance: Evaluate our model's accuracy on validation data and score it against test data which it has never seen in training.<br>\n",
        "6. Explore options for improving our model: Is our dataset the right size? Is our model too complex or too simple? <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b2IL0uT19_3A"
      },
      "outputs": [],
      "source": [
        "#importing the required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import os\n",
        "# matplotlib complains about the behaviour of librosa.display, so we'll ignore those warnings:\n",
        "import warnings; warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu8VPhDmL9hC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Feature Extraction\n",
        "We're going to take full advantage of librosa, a Python library enabling audio analysis and feature extraction.\n",
        "Librosa abstracts away all the math and most of the details of mel spectrorgams, chromagrams, and MFCC.\n",
        "Although closely related, we're going to take the Mel Spectrogram, MFCC, and chromagrams of each audio file as separate features to try\n",
        "and have bit more discriminatory power between samples. <br>\n",
        "\n",
        "Let's build our feature extraction functions to get a chromagram, a mel spectorgram, and MFC coefficients for each of our audio files. Because the chromagram, mel spectrogram and MFCCs are calculated on audio frames produced by STFT, we're going to get a matrix back from each function, so we'll take the mean of those matrices to produce a single feature array for each feature and each audio sample, i.e. 3 feature arrays per audio sample.\n",
        "\n",
        "**Chromagram**: Will produce 12 features; One for each of 12 pitch classes\n",
        "\n",
        "**Mel Spectrogram**: Will produce 128 features; We've defined the number of mel frequency bands at n_mels=128\n",
        "\n",
        "**MFCC**: Will produce 40 MFCCs; I've set the number of coefficients to return at n_mfcc=40 which I found to work well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qTe93WYTL9hD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def feature_chromagram(waveform, sample_rate):\n",
        "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
        "    stft_spectrogram=np.abs(librosa.stft(waveform))\n",
        "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T,axis=0)\n",
        "    return chromagram\n",
        "\n",
        "def feature_melspectrogram(waveform, sample_rate):\n",
        "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
        "    melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128, fmax=8000).T,axis=0)\n",
        "    return melspectrogram\n",
        "\n",
        "def feature_mfcc(waveform, sample_rate):\n",
        "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # 40 filterbanks = 40 coefficients\n",
        "    mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "    return mfc_coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjFXIfC2L9hD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We're going to wrap our feature extraction functions so we only have to load each audio file once. After extracting our 3 audio features as NumPy arrays representing a time series, we're going to\n",
        "stack them horizontally to create a single feature array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "f-UK-s6Rhl1F"
      },
      "outputs": [],
      "source": [
        "def preprocess_waveform(waveform):\n",
        "    # If the waveform has 2 channels (stereo), convert it to mono\n",
        "    if len(waveform.shape) > 1:\n",
        "        waveform = librosa.to_mono(waveform)\n",
        "    return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xPMw9ijJL9hE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_features(file):\n",
        "    # load an individual soundfile\n",
        "     with soundfile.SoundFile(file) as audio:\n",
        "        waveform = audio.read(dtype=\"float32\")\n",
        "        sample_rate = audio.samplerate\n",
        "        # make sure the file is mono channel audio\n",
        "        waveform = preprocess_waveform(waveform)\n",
        "        # compute features of soundfile\n",
        "        chromagram = feature_chromagram(waveform, sample_rate)\n",
        "        melspectrogram = feature_melspectrogram(waveform, sample_rate)\n",
        "        mfc_coefficients = feature_mfcc(waveform, sample_rate)\n",
        "\n",
        "        feature_matrix=np.array([])\n",
        "\n",
        "        # Check the shape of chromagram\n",
        "        if chromagram.ndim > 1 and chromagram.shape[1] > 1:\n",
        "            #print(f\"Returning zero vector for chromagram size: {file} (shape: {chromagram.shape})\")\n",
        "            chromagram = np.zeros((12,))  # Return a zero vector of size (12,)\n",
        "\n",
        "        # Check the shape of mel spectrogram\n",
        "        if melspectrogram.ndim > 1 and melspectrogram.shape[1] > 1:\n",
        "            #print(f\"Returning zero vector for mel spectrogram size: {file} (shape: {melspectrogram.shape})\")\n",
        "            melspectrogram = np.zeros((128,))  # Return a zero vector of size (128,)\n",
        "\n",
        "        # Check the shape of MFCC coefficients\n",
        "        if mfc_coefficients.ndim > 1 and mfc_coefficients.shape[1] > 1:\n",
        "            #print(f\"Returning zero vector for MFCC size: {file} (shape: {mfc_coefficients.shape})\")\n",
        "            mfc_coefficients = np.zeros((40,))  # Return a zero vector of size (40,)\n",
        "\n",
        "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
        "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients))\n",
        "\n",
        "        return feature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-C6g6psL9hE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Load the Dataset and Compute Features\n",
        "We have to understand the labelling of the RAVDESS dataset to find the ground truth emotion for each sample.\n",
        "Each file is labelled with 7 numbers delimited by a \"-\".\n",
        "Most of the numbers describe metadata about the audio samples such as their format (video and/or audio),\n",
        "whether the audio is a song or statement, which of two statements is being read and by which actor.\n",
        "\n",
        "The third and fourth numbers pertain to the emotional quality of each sample. The third number is in the range of 1-8 with each number representing an emotion.\n",
        "The fourth number is either 1 or 2, representing normal (1) or strong (2) emotional intensity.\n",
        "\n",
        "We're going to define a dictionary based on the third number (emotion) and assign an emotion to each number as specified by the RAVDESS dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z-Pu_fB7L9hF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#Emotions in the RAVDESS dataset\n",
        "emotions_dict ={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTlsUOwXL9hF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Finally, let's load our entire dataset and compute the features of each audio file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mAh2AYMpL9hF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "\n",
        "def load_data():\n",
        "    X,y=[],[]\n",
        "    count = 0\n",
        "    # for file in glob.glob(\"My Audio Data/*/*.wav\"):\n",
        "    # inputting my new audio data\n",
        "    for file in glob.glob(\"My Audio Data/*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=emotions_dict[file_name.split(\"-\")[2]]\n",
        "        features = get_features(file)\n",
        "        X.append(features)\n",
        "        y.append(emotion)\n",
        "        count += 1\n",
        "        # '\\r' + end='' results in printing over same line\n",
        "        print('\\r' + f' Processed {count}/{1440} audio samples',end=' ')\n",
        "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cf8q4K5L9hG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Compute the feature matrix and read the emotion labels for the entire dataset.\n",
        "Note that our regressor (independent/explanatory variable), usually denoted X, is named 'features', and our regressand (dependent variable), usually denoted y, is named 'emotions'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeKE591aL9hG",
        "outputId": "d34ab1f9-d263-493d-99bc-23cd4099a0ed",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processed 1439/1440 audio samples "
          ]
        }
      ],
      "source": [
        "features, emotions = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUa7RAuL9hG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's see what the features we extracted look like, **also for saving both the features matrix as well as emotions array, we need to convert them to pandas dataframe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "mzxX583yL9hG",
        "outputId": "bb7b68eb-071f-4b11-b890-8d781bb3ea95",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Audio samples represented: 1439\n",
            "Numerical features extracted per sample: 180\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.552245</td>\n",
              "      <td>0.504338</td>\n",
              "      <td>0.512779</td>\n",
              "      <td>0.550540</td>\n",
              "      <td>0.572176</td>\n",
              "      <td>0.615567</td>\n",
              "      <td>0.603708</td>\n",
              "      <td>0.573230</td>\n",
              "      <td>0.593454</td>\n",
              "      <td>0.603834</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.543538</td>\n",
              "      <td>-0.912676</td>\n",
              "      <td>-1.747537</td>\n",
              "      <td>-2.299447</td>\n",
              "      <td>-0.025205</td>\n",
              "      <td>2.449470</td>\n",
              "      <td>2.116510</td>\n",
              "      <td>3.361054</td>\n",
              "      <td>2.086241</td>\n",
              "      <td>0.376519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.550732</td>\n",
              "      <td>0.508484</td>\n",
              "      <td>0.466810</td>\n",
              "      <td>0.438485</td>\n",
              "      <td>0.428218</td>\n",
              "      <td>0.445352</td>\n",
              "      <td>0.444528</td>\n",
              "      <td>0.444478</td>\n",
              "      <td>0.500419</td>\n",
              "      <td>0.551673</td>\n",
              "      <td>...</td>\n",
              "      <td>2.333152</td>\n",
              "      <td>3.619088</td>\n",
              "      <td>0.793563</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.660723</td>\n",
              "      <td>-0.254914</td>\n",
              "      <td>-0.967536</td>\n",
              "      <td>0.378563</td>\n",
              "      <td>-0.338074</td>\n",
              "      <td>-0.463241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.672768</td>\n",
              "      <td>0.601451</td>\n",
              "      <td>0.548317</td>\n",
              "      <td>0.477806</td>\n",
              "      <td>0.453192</td>\n",
              "      <td>0.516665</td>\n",
              "      <td>0.570100</td>\n",
              "      <td>0.599750</td>\n",
              "      <td>0.647746</td>\n",
              "      <td>0.692148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283253</td>\n",
              "      <td>0.218117</td>\n",
              "      <td>-1.928165</td>\n",
              "      <td>-2.486516</td>\n",
              "      <td>-0.530146</td>\n",
              "      <td>0.354905</td>\n",
              "      <td>0.463640</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>-2.091462</td>\n",
              "      <td>-1.899596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.602309</td>\n",
              "      <td>0.605448</td>\n",
              "      <td>0.558788</td>\n",
              "      <td>0.529441</td>\n",
              "      <td>0.546190</td>\n",
              "      <td>0.568049</td>\n",
              "      <td>0.566027</td>\n",
              "      <td>0.523464</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.552809</td>\n",
              "      <td>...</td>\n",
              "      <td>2.176565</td>\n",
              "      <td>3.904047</td>\n",
              "      <td>0.980935</td>\n",
              "      <td>1.160923</td>\n",
              "      <td>-0.694902</td>\n",
              "      <td>-0.969963</td>\n",
              "      <td>-0.234802</td>\n",
              "      <td>0.941701</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.430566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.556555</td>\n",
              "      <td>0.503434</td>\n",
              "      <td>0.473118</td>\n",
              "      <td>0.479222</td>\n",
              "      <td>0.495470</td>\n",
              "      <td>0.522490</td>\n",
              "      <td>0.532196</td>\n",
              "      <td>0.540681</td>\n",
              "      <td>0.545616</td>\n",
              "      <td>0.512521</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.306031</td>\n",
              "      <td>0.050261</td>\n",
              "      <td>-0.992538</td>\n",
              "      <td>-0.213367</td>\n",
              "      <td>-0.351018</td>\n",
              "      <td>-1.462261</td>\n",
              "      <td>-2.454565</td>\n",
              "      <td>-1.044800</td>\n",
              "      <td>-0.298479</td>\n",
              "      <td>0.118208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>0.664177</td>\n",
              "      <td>0.633587</td>\n",
              "      <td>0.590743</td>\n",
              "      <td>0.601933</td>\n",
              "      <td>0.623831</td>\n",
              "      <td>0.643299</td>\n",
              "      <td>0.671203</td>\n",
              "      <td>0.668196</td>\n",
              "      <td>0.683204</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>...</td>\n",
              "      <td>5.234397</td>\n",
              "      <td>5.000974</td>\n",
              "      <td>4.120271</td>\n",
              "      <td>3.053776</td>\n",
              "      <td>2.119890</td>\n",
              "      <td>2.965642</td>\n",
              "      <td>2.442994</td>\n",
              "      <td>2.014850</td>\n",
              "      <td>3.167833</td>\n",
              "      <td>4.008449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>0.697806</td>\n",
              "      <td>0.658317</td>\n",
              "      <td>0.614802</td>\n",
              "      <td>0.639169</td>\n",
              "      <td>0.717383</td>\n",
              "      <td>0.743908</td>\n",
              "      <td>0.781170</td>\n",
              "      <td>0.749796</td>\n",
              "      <td>0.722322</td>\n",
              "      <td>0.715267</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.211285</td>\n",
              "      <td>0.060299</td>\n",
              "      <td>-1.140080</td>\n",
              "      <td>0.446977</td>\n",
              "      <td>0.614657</td>\n",
              "      <td>0.430341</td>\n",
              "      <td>0.332445</td>\n",
              "      <td>-0.197219</td>\n",
              "      <td>0.797524</td>\n",
              "      <td>2.606972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>0.628023</td>\n",
              "      <td>0.596604</td>\n",
              "      <td>0.643935</td>\n",
              "      <td>0.655228</td>\n",
              "      <td>0.659819</td>\n",
              "      <td>0.725635</td>\n",
              "      <td>0.808151</td>\n",
              "      <td>0.814369</td>\n",
              "      <td>0.771093</td>\n",
              "      <td>0.722508</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.125915</td>\n",
              "      <td>0.213711</td>\n",
              "      <td>-1.729091</td>\n",
              "      <td>0.776150</td>\n",
              "      <td>0.564957</td>\n",
              "      <td>2.507703</td>\n",
              "      <td>0.997836</td>\n",
              "      <td>0.844502</td>\n",
              "      <td>-0.108339</td>\n",
              "      <td>2.037053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>0.630163</td>\n",
              "      <td>0.652205</td>\n",
              "      <td>0.637307</td>\n",
              "      <td>0.684746</td>\n",
              "      <td>0.764458</td>\n",
              "      <td>0.795282</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>0.802949</td>\n",
              "      <td>0.774911</td>\n",
              "      <td>0.734925</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.221260</td>\n",
              "      <td>-0.709731</td>\n",
              "      <td>-1.652230</td>\n",
              "      <td>-1.931097</td>\n",
              "      <td>-1.620286</td>\n",
              "      <td>1.130693</td>\n",
              "      <td>0.274002</td>\n",
              "      <td>2.252403</td>\n",
              "      <td>2.537369</td>\n",
              "      <td>3.869971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>0.619704</td>\n",
              "      <td>0.639446</td>\n",
              "      <td>0.650099</td>\n",
              "      <td>0.693850</td>\n",
              "      <td>0.721269</td>\n",
              "      <td>0.756798</td>\n",
              "      <td>0.789992</td>\n",
              "      <td>0.768492</td>\n",
              "      <td>0.766670</td>\n",
              "      <td>0.698196</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.135965</td>\n",
              "      <td>-0.554937</td>\n",
              "      <td>-1.294902</td>\n",
              "      <td>-0.990537</td>\n",
              "      <td>-0.618531</td>\n",
              "      <td>0.289876</td>\n",
              "      <td>-0.120279</td>\n",
              "      <td>0.519724</td>\n",
              "      <td>1.181626</td>\n",
              "      <td>2.576108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1439 rows × 180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     0.552245  0.504338  0.512779  0.550540  0.572176  0.615567  0.603708   \n",
              "1     0.550732  0.508484  0.466810  0.438485  0.428218  0.445352  0.444528   \n",
              "2     0.672768  0.601451  0.548317  0.477806  0.453192  0.516665  0.570100   \n",
              "3     0.602309  0.605448  0.558788  0.529441  0.546190  0.568049  0.566027   \n",
              "4     0.556555  0.503434  0.473118  0.479222  0.495470  0.522490  0.532196   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1434  0.664177  0.633587  0.590743  0.601933  0.623831  0.643299  0.671203   \n",
              "1435  0.697806  0.658317  0.614802  0.639169  0.717383  0.743908  0.781170   \n",
              "1436  0.628023  0.596604  0.643935  0.655228  0.659819  0.725635  0.808151   \n",
              "1437  0.630163  0.652205  0.637307  0.684746  0.764458  0.795282  0.819000   \n",
              "1438  0.619704  0.639446  0.650099  0.693850  0.721269  0.756798  0.789992   \n",
              "\n",
              "           7         8         9    ...       170       171       172  \\\n",
              "0     0.573230  0.593454  0.603834  ... -3.543538 -0.912676 -1.747537   \n",
              "1     0.444478  0.500419  0.551673  ...  2.333152  3.619088  0.793563   \n",
              "2     0.599750  0.647746  0.692148  ...  0.283253  0.218117 -1.928165   \n",
              "3     0.523464  0.526789  0.552809  ...  2.176565  3.904047  0.980935   \n",
              "4     0.540681  0.545616  0.512521  ... -1.306031  0.050261 -0.992538   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1434  0.668196  0.683204  0.658824  ...  5.234397  5.000974  4.120271   \n",
              "1435  0.749796  0.722322  0.715267  ... -1.211285  0.060299 -1.140080   \n",
              "1436  0.814369  0.771093  0.722508  ... -1.125915  0.213711 -1.729091   \n",
              "1437  0.802949  0.774911  0.734925  ... -2.221260 -0.709731 -1.652230   \n",
              "1438  0.768492  0.766670  0.698196  ... -1.135965 -0.554937 -1.294902   \n",
              "\n",
              "           173       174       175       176       177       178       179  \n",
              "0    -2.299447 -0.025205  2.449470  2.116510  3.361054  2.086241  0.376519  \n",
              "1     0.743431  0.660723 -0.254914 -0.967536  0.378563 -0.338074 -0.463241  \n",
              "2    -2.486516 -0.530146  0.354905  0.463640  0.581006 -2.091462 -1.899596  \n",
              "3     1.160923 -0.694902 -0.969963 -0.234802  0.941701  0.695973  0.430566  \n",
              "4    -0.213367 -0.351018 -1.462261 -2.454565 -1.044800 -0.298479  0.118208  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "1434  3.053776  2.119890  2.965642  2.442994  2.014850  3.167833  4.008449  \n",
              "1435  0.446977  0.614657  0.430341  0.332445 -0.197219  0.797524  2.606972  \n",
              "1436  0.776150  0.564957  2.507703  0.997836  0.844502 -0.108339  2.037053  \n",
              "1437 -1.931097 -1.620286  1.130693  0.274002  2.252403  2.537369  3.869971  \n",
              "1438 -0.990537 -0.618531  0.289876 -0.120279  0.519724  1.181626  2.576108  \n",
              "\n",
              "[1439 rows x 180 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f'\\nAudio samples represented: {features.shape[0]}')\n",
        "print(f'Numerical features extracted per sample: {features.shape[1]}')\n",
        "features_df = pd.DataFrame(features) # make it pretty for display\n",
        "\n",
        "\n",
        "#making dataframe for emotions as well\n",
        "emotions_df = pd.DataFrame(emotions) # make it pretty for display\n",
        "\n",
        "features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq083zLEL9hH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We have a matrix of dim 1435 x 180. Looks good - 1435 audio samples, one per row, with a series of\n",
        "180 numerical features for each sample.\n",
        "\n",
        "**Each of the 1435 feature arrays has 180 features composed of 12 chromagram pitch classes + 128 mel spectrogram bands + 40 MFC coefficients.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF2SggHTDqbQ"
      },
      "source": [
        "Now we will save our features matrix and emotions array in excel file we dont have to compute them everytime we run the notebook, we can just load them from the excel file whenever required. Make sure to change the path to according to your drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "u6H8hc6gDtbp"
      },
      "outputs": [],
      "source": [
        "features_df.to_csv('featuresRavdess.csv')\n",
        "emotions_df.to_csv('emotionsRavdess.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpE5m-5aEyoB"
      },
      "source": [
        "## Load pre-saved Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPB7dCqEotR"
      },
      "source": [
        "Once saved you only need to load them later by running the cell below, and **skip every cell above** except for the one in which we import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-8nlJCESEn56"
      },
      "outputs": [],
      "source": [
        "features=pd.read_csv('featuresRavdess.csv',index_col=0)\n",
        "emotions=pd.read_csv('emotionsRavdess.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2dXjybD1NM"
      },
      "source": [
        "let's see if they have been loaded correctly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "B5Xo1SVMD0qR",
        "outputId": "11d55ba5-56c0-4c71-dc44-04907d2e0186"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.552245</td>\n",
              "      <td>0.504338</td>\n",
              "      <td>0.512779</td>\n",
              "      <td>0.550540</td>\n",
              "      <td>0.572176</td>\n",
              "      <td>0.615567</td>\n",
              "      <td>0.603708</td>\n",
              "      <td>0.573230</td>\n",
              "      <td>0.593454</td>\n",
              "      <td>0.603834</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.543538</td>\n",
              "      <td>-0.912676</td>\n",
              "      <td>-1.747537</td>\n",
              "      <td>-2.299447</td>\n",
              "      <td>-0.025205</td>\n",
              "      <td>2.449470</td>\n",
              "      <td>2.116510</td>\n",
              "      <td>3.361054</td>\n",
              "      <td>2.086241</td>\n",
              "      <td>0.376519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.550732</td>\n",
              "      <td>0.508485</td>\n",
              "      <td>0.466810</td>\n",
              "      <td>0.438485</td>\n",
              "      <td>0.428218</td>\n",
              "      <td>0.445352</td>\n",
              "      <td>0.444528</td>\n",
              "      <td>0.444478</td>\n",
              "      <td>0.500419</td>\n",
              "      <td>0.551673</td>\n",
              "      <td>...</td>\n",
              "      <td>2.333152</td>\n",
              "      <td>3.619088</td>\n",
              "      <td>0.793563</td>\n",
              "      <td>0.743431</td>\n",
              "      <td>0.660723</td>\n",
              "      <td>-0.254914</td>\n",
              "      <td>-0.967536</td>\n",
              "      <td>0.378563</td>\n",
              "      <td>-0.338074</td>\n",
              "      <td>-0.463241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.672768</td>\n",
              "      <td>0.601451</td>\n",
              "      <td>0.548317</td>\n",
              "      <td>0.477806</td>\n",
              "      <td>0.453192</td>\n",
              "      <td>0.516665</td>\n",
              "      <td>0.570100</td>\n",
              "      <td>0.599750</td>\n",
              "      <td>0.647746</td>\n",
              "      <td>0.692148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.283253</td>\n",
              "      <td>0.218117</td>\n",
              "      <td>-1.928165</td>\n",
              "      <td>-2.486515</td>\n",
              "      <td>-0.530146</td>\n",
              "      <td>0.354905</td>\n",
              "      <td>0.463640</td>\n",
              "      <td>0.581006</td>\n",
              "      <td>-2.091462</td>\n",
              "      <td>-1.899596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.602309</td>\n",
              "      <td>0.605448</td>\n",
              "      <td>0.558788</td>\n",
              "      <td>0.529441</td>\n",
              "      <td>0.546190</td>\n",
              "      <td>0.568049</td>\n",
              "      <td>0.566027</td>\n",
              "      <td>0.523464</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.552809</td>\n",
              "      <td>...</td>\n",
              "      <td>2.176565</td>\n",
              "      <td>3.904047</td>\n",
              "      <td>0.980935</td>\n",
              "      <td>1.160923</td>\n",
              "      <td>-0.694902</td>\n",
              "      <td>-0.969963</td>\n",
              "      <td>-0.234802</td>\n",
              "      <td>0.941701</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.430566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.556555</td>\n",
              "      <td>0.503434</td>\n",
              "      <td>0.473118</td>\n",
              "      <td>0.479222</td>\n",
              "      <td>0.495470</td>\n",
              "      <td>0.522490</td>\n",
              "      <td>0.532196</td>\n",
              "      <td>0.540681</td>\n",
              "      <td>0.545616</td>\n",
              "      <td>0.512521</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.306031</td>\n",
              "      <td>0.050261</td>\n",
              "      <td>-0.992538</td>\n",
              "      <td>-0.213367</td>\n",
              "      <td>-0.351018</td>\n",
              "      <td>-1.462261</td>\n",
              "      <td>-2.454565</td>\n",
              "      <td>-1.044800</td>\n",
              "      <td>-0.298479</td>\n",
              "      <td>0.118208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.552245  0.504338  0.512779  0.550540  0.572176  0.615567  0.603708   \n",
              "1  0.550732  0.508485  0.466810  0.438485  0.428218  0.445352  0.444528   \n",
              "2  0.672768  0.601451  0.548317  0.477806  0.453192  0.516665  0.570100   \n",
              "3  0.602309  0.605448  0.558788  0.529441  0.546190  0.568049  0.566027   \n",
              "4  0.556555  0.503434  0.473118  0.479222  0.495470  0.522490  0.532196   \n",
              "\n",
              "          7         8         9  ...       170       171       172       173  \\\n",
              "0  0.573230  0.593454  0.603834  ... -3.543538 -0.912676 -1.747537 -2.299447   \n",
              "1  0.444478  0.500419  0.551673  ...  2.333152  3.619088  0.793563  0.743431   \n",
              "2  0.599750  0.647746  0.692148  ...  0.283253  0.218117 -1.928165 -2.486515   \n",
              "3  0.523464  0.526789  0.552809  ...  2.176565  3.904047  0.980935  1.160923   \n",
              "4  0.540681  0.545616  0.512521  ... -1.306031  0.050261 -0.992538 -0.213367   \n",
              "\n",
              "        174       175       176       177       178       179  \n",
              "0 -0.025205  2.449470  2.116510  3.361054  2.086241  0.376519  \n",
              "1  0.660723 -0.254914 -0.967536  0.378563 -0.338074 -0.463241  \n",
              "2 -0.530146  0.354905  0.463640  0.581006 -2.091462 -1.899596  \n",
              "3 -0.694902 -0.969963 -0.234802  0.941701  0.695973  0.430566  \n",
              "4 -0.351018 -1.462261 -2.454565 -1.044800 -0.298479  0.118208  \n",
              "\n",
              "[5 rows x 180 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esInVDq7L9hT"
      },
      "source": [
        "Let's see the class balance of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Si3OFQe7L9hU",
        "outputId": "c2f3f5ab-1c0c-4d27-e712-ceb075e55617"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2520x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot emotions\n",
        "plt.figure(figsize=(35,4))\n",
        "plt.subplot(1,3,1)\n",
        "#np.unique returns ordered list of unique elements and count of each element\n",
        "emotion_list, count = np.unique(emotions, return_counts=True)\n",
        "plt.bar(x=range(8), height=count)\n",
        "plt.xticks(ticks=range(8), labels = [emotion for emotion in emotion_list],fontsize=10)\n",
        "plt.xlabel('Emotion')\n",
        "plt.tick_params(labelsize=16)\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw_wKC8WL9hV"
      },
      "source": [
        "**Great, the classes appear to be balanced. That makes the task easier.** All emotions _except_ the neutral class have a \"strong\" intensity so there are half as many neutral samples. That might have an impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhtlYshOL9hV"
      },
      "source": [
        "### Feature Scaling\n",
        "To properly train most machine learning models on _most_ datasets, we first need to scale our features. **This is crucial for models which compute distances between data, and especially critical for DNNs**: If there is a difference in the variance of features simply because of their possible range of values, then a model will learn that the features with the greatest variance are the most important. However, **differences in the variance of unscaled features belonging to different and unknown distributions is an inappropriate measure of importance.** Let's check our features' properties:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IUm1DLwL9hW",
        "outputId": "dbca9b9e-bae9-4732-e193-ce627f60bcc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12 Chromagram features:           min = 0.310,     max = 1.000,     mean = 0.667,     deviation = 0.087\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 149.208,     mean = 0.187,     deviation = 1.598\n",
            "\n",
            "40 MFCC features:                 min = -1131.371,    max = 115.126,    mean = -14.674,    deviation = 98.873\n"
          ]
        }
      ],
      "source": [
        "# We would usually use df.describe(), but it provides a bit of a mess of information we don't need at the moment.\n",
        "def print_features(df):\n",
        "    # Check chromagram feature values\n",
        "    features_df_chromagram = df.loc[:,:11]\n",
        "    chroma_min = features_df_chromagram.min().min()\n",
        "    chroma_max = features_df_chromagram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    chroma_mean = features_df_chromagram.stack().mean()\n",
        "    chroma_stdev = features_df_chromagram.stack().std()\n",
        "    print(f'12 Chromagram features:       \\\n",
        "    min = {chroma_min:.3f}, \\\n",
        "    max = {chroma_max:.3f}, \\\n",
        "    mean = {chroma_mean:.3f}, \\\n",
        "    deviation = {chroma_stdev:.3f}')\n",
        "\n",
        "    # Check mel spectrogram feature values\n",
        "    features_df_melspectrogram = df.loc[:,12:139]\n",
        "    mel_min = features_df_melspectrogram.min().min()\n",
        "    mel_max = features_df_melspectrogram.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mel_mean = features_df_melspectrogram.stack().mean()\n",
        "    mel_stdev = features_df_melspectrogram.stack().std()\n",
        "    print(f'\\n128 Mel Spectrogram features: \\\n",
        "    min = {mel_min:.3f}, \\\n",
        "    max = {mel_max:.3f}, \\\n",
        "    mean = {mel_mean:.3f}, \\\n",
        "    deviation = {mel_stdev:.3f}')\n",
        "\n",
        "    # Check MFCC feature values\n",
        "    features_df_mfcc = df.loc[:,140:179]\n",
        "    mfcc_min = features_df_mfcc.min().min()\n",
        "    mfcc_max = features_df_mfcc.max().max()\n",
        "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
        "    mfcc_mean = features_df_mfcc.stack().mean()\n",
        "    mfcc_stdev = features_df_mfcc.stack().std()\n",
        "    print(f'\\n40 MFCC features:             \\\n",
        "    min = {mfcc_min:.3f},\\\n",
        "    max = {mfcc_max:.3f},\\\n",
        "    mean = {mfcc_mean:.3f},\\\n",
        "    deviation = {mfcc_stdev:.3f}')\n",
        "\n",
        "print_features(features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ha1FxBSCpqA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFgQlYSlL9hW"
      },
      "source": [
        "**There's an obvious imbalance in the variance our features; Our features indeed belong to very different distributions:** our MFC coefficients' deviation is greater than the other features by orders of magnitude. That does not mean MFC coefficients are the most important feature, but rather it is a property of the way they are computed.  We will certainly need to scale this feature set.\n",
        "\n",
        "We have the choice of sklearn's StandardScaler and MinMaxScaler. Standard scaling subtracts the mean of each feature and divides it by the standard deviation of that feature, producing features with mean at zero and unit variance - that is, a variance and standard deviation of 1. Min-Max scaling transforms each feature to be within a bounded interval that we specify.\n",
        "\n",
        "In practice, **MinMax scaling is especially useful when we know our features should be in a bounded interval**, such as pixel values in [0,255], while **standard scaling is perhaps more practical for features with unknown distributions** because centering the features at zero-mean with a standard deviation of 1 means extreme values will have less of an impact on the model's learned weights, i.e. the model is less sensitive to outliers.\n",
        "\n",
        "We'll create MinMax scaled features as well so we can give them a try later on to confirm that standard scaling is better in the absence of knowledge on the appropriate distribution for a dataset's features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_BCAYVEUL9hW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_scaled = features\n",
        "features_scaled = scaler.fit_transform(features_scaled)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# keep our unscaled features just in case we need to process them alternatively\n",
        "features_minmax = features\n",
        "features_minmax = scaler.fit_transform(features_minmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ATx5oNL9hX"
      },
      "source": [
        "Make sure our features are properly scaled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRuHQkKL9hX",
        "outputId": "4bd31c3c-47ed-4f16-d45c-0767ceb48da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mStandard Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = -3.896,     max = 4.368,     mean = -0.000,     deviation = 1.000\n",
            "\n",
            "128 Mel Spectrogram features:     min = -0.474,     max = 36.531,     mean = 0.000,     deviation = 1.000\n",
            "\n",
            "40 MFCC features:                 min = -4.803,    max = 6.238,    mean = -0.000,    deviation = 1.000\n",
            "\n",
            "\n",
            "\u001b[1mMinMax Scaling:\n",
            "\u001b[0m\n",
            "12 Chromagram features:           min = 0.000,     max = 1.000,     mean = 0.472,     deviation = 0.145\n",
            "\n",
            "128 Mel Spectrogram features:     min = 0.000,     max = 1.000,     mean = 0.015,     deviation = 0.061\n",
            "\n",
            "40 MFCC features:                 min = 0.000,    max = 1.000,    mean = 0.412,    deviation = 0.169\n"
          ]
        }
      ],
      "source": [
        "print('\\033[1m'+'Standard Scaling:\\n'+'\\033[0m')\n",
        "features_scaled_df = pd.DataFrame(features_scaled)\n",
        "print_features(features_scaled_df)\n",
        "\n",
        "print('\\n\\n\\033[1m'+'MinMax Scaling:\\n'+'\\033[0m')\n",
        "features_minmax_df = pd.DataFrame(features_minmax)\n",
        "print_features(features_minmax_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfzIh7DL9hX"
      },
      "source": [
        "Perfect. Zero mean and unit variance for standard scaling and in the range [0,1] for MinMax scaling - a default when we don't specify values. We can now move on to building predictive models for these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5tXewLEL9hX"
      },
      "source": [
        "## Classical Machine Learning Models\n",
        "\n",
        "\n",
        "Classical machine learning models encompass a broad range of algorithms that have been foundational to the field's development and are still widely used for various predictive tasks. These models can be broadly categorized into supervised and unsupervised learning methods, each suited for different kinds of data and objectives.\n",
        "\n",
        "We will be looking into few popular Machine Learning Algorithms such as Support Vector Machine(SVM), K-Nearest Neighbors and Random Forest Classifier. There are many other classical models with their own strengths and weaknesses, and the choice of model depends on the specific requirements of the task, including the nature of the data, the complexity of the problem, and the computational efficiency required. Despite the rise of deep learning, classical machine learning models remain vital tools in a data scientist's arsenal due to their efficiency, interpretability, and strong performance in many scenarios.\n",
        "\n",
        "The use of classic machine learning method is due to the small size of our dataset; Some of the most robust models such as Support vector (machine) classifiers **(SVC) and k-Nearest-Neighbour classifiers (kNN) are particularly suited to smaller datasets and fall apart with huge datasets.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwU-jXSGL9hY"
      },
      "source": [
        "### Training: The 80/20 Split and Validation\n",
        "In order to compare models, we'll have to evaluate their performance. The simplest method to do so is to train a model on a portion of our dataset and test it on the remainder. We'll use sklearn's train_test_split to create a standard 80/20 train/test split. The model is fit on 80% of\n",
        "the data and tested for performance against 20% of the data, which it has never seen in training - also called the hold-out set.\n",
        " <img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/Capture2.PNG?raw=true\" width=\"800\">\n",
        "\n",
        "More accurately, the proper modality for training and scoring a model is to\n",
        "1. Fit/train our model on a _training_ set,\n",
        "2. Evaluate the model on a _validation_ set to tune the hyperparameters for better performance,\n",
        "3. Finally score our model's true performance - its **generalizability** - against a _test_ set, aka the hold-out set.\n",
        "4. Repeat from 2. **Do not tune the model to score well on the test set**. Only evaluate on test-set once.\n",
        "\n",
        "Different set ratios are used in this approach - a usual example is 60/20/20 train/validation/test.For simplicity, we're going to start with an 80/20 train/test split. The model will be trained on all the training data, and we will check its performance on the test data. We'll skip validation for now.\n",
        "\n",
        " <img src=\"https://github.com/IAT-ExploringAI-2024/Week3-ClassicML/blob/main/images/traintestsplit.PNG?raw=true\" width=\"800\">\n",
        "\n",
        "Define unscaled and scaled training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8jGN4ROVL9hY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "############# Unscaled test/train set #############\n",
        "X_train, X_test, y_train, y_test =train_test_split(\n",
        "    features,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "############ Standard Scaled test/train set ###########\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_scaled, X_test_scaled, _, _ = train_test_split(\n",
        "    features_scaled,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "############# MinMax Scaled test/train set ###############\n",
        "# The labels/classes (y_train, y_test) never change, keep old values\n",
        "X_train_minmax, X_test_minmax, _, _ = train_test_split(\n",
        "    features_scaled,\n",
        "    emotions,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Mh0RrEL9hY"
      },
      "source": [
        "### Comparing Models\n",
        "We'll try each off-the-shelf machine learning model from sklearn and pick a few to explore, since these models will train near instantly on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "iLPeyR7vL9he",
        "outputId": "067b25d8-9f5e-4d9c-f35f-cefcda65090a",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier</th>\n",
              "      <th>Accuracy Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>58.68%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>56.25%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVC</td>\n",
              "      <td>52.43%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVC RBF kernel</td>\n",
              "      <td>50.35%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>39.93%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>32.29%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AdaBoostClassifier</td>\n",
              "      <td>29.86%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>QuadraticDiscriminantAnalysis</td>\n",
              "      <td>26.04%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Classifier Accuracy Score\n",
              "4         RandomForestClassifier         58.68%\n",
              "0           KNeighborsClassifier         56.25%\n",
              "1                            SVC         52.43%\n",
              "2                 SVC RBF kernel         50.35%\n",
              "3         DecisionTreeClassifier         39.93%\n",
              "6                     GaussianNB         32.29%\n",
              "5             AdaBoostClassifier         29.86%\n",
              "7  QuadraticDiscriminantAnalysis         26.04%"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "classification_models = [\n",
        "    KNeighborsClassifier(),#(3),\n",
        "    SVC(kernel='linear'),#, C=0.025),\n",
        "    SVC(kernel='rbf'),\n",
        "    DecisionTreeClassifier(),#max_depth=5),\n",
        "    RandomForestClassifier(),#max_depth=5, n_estimators=10, max_features=1),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis()]\n",
        "\n",
        "scores = []\n",
        "for model in classification_models:\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    score = model.score(X_test_scaled, y_test)\n",
        "    model_name = type(model).__name__\n",
        "    if model_name=='SVC' and model.kernel=='rbf': model_name+=' RBF kernel'\n",
        "    scores.append((model_name,(f'{100*score:.2f}%')))\n",
        "# Make it pretty\n",
        "scores_df = pd.DataFrame(scores,columns=['Classifier','Accuracy Score'])\n",
        "scores_df.sort_values(by='Accuracy Score',axis=0,ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrD98CpPL9he"
      },
      "source": [
        "Let's pick the top three - Random Forests, SVC, and kNN - and take a closer look at each of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBNoEolL9hf"
      },
      "source": [
        "### The Support Vector Machine Classifier\n",
        "\n",
        "We'll go in chronological order. First is the support vector machine classifier (SVC) - a model from the 60s. SVMs are models quick to train for this task and best suited to small datasets due to its quadratic time complexity w.r.t. size of the training dataset (# of training samples). This is also the reason it breaks down with larger datasets since it becomes very expensive to train.\n",
        "\n",
        "The idea behind SVMs on which the SVC model is based is to find a separating hyperplane - a subspace with dimension one less than that of the feature space - for points in our feature space; i.e. for a 3D space, a hyperplane is a regular plane, in 2D, a line. This idea extends to n dimensions. If points are separable by a hyperplane, they are said to be linearly separable. **Since there are infinite possible separating hyperplanes for any linearly separable feature space, an SVM computes which points are closest to each such hyperplane and uses them to construct a _support vector_. The SVM picks the hyperplane which maximizes the distance - _margin_ - to each support vector.** In this way, we maximize the separating ability of the chosen hyperplane.\n",
        "\n",
        "The core of SVMs is the kernel. We could map all new points from our input space, where they were not separable by a hyperplane, to a higher dimension in which we have found a hyperplane to separate the points in that space. However, that would be extremely computationally expensive for data that needs to be mapped to much higher dimensions. Instead, we **compute the hyperplane in the higher dimension on our training data and map the hyperplane back to the lower-dimension input space to use for classifying our data. This is the _kernel trick_, whereby the kernel (function) enables us to compute distances to new points in the input space without transforming each to the higher dimensional space - drastically reducing the computational complexity of the SVM.**\n",
        "\n",
        " <img src=\"https://github.com/IAT-ExploringAI-2024/Week3-Machine_Learning/blob/main/images/kernel1.png?raw=true\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VTE6HjL9hf"
      },
      "source": [
        "A linear kernel should always be tested because **a linear kernel is much faster to train than a non-linear kernel**; however, properly tuned, a non-linear kernel often provides the best possible predictive performance. **RBF (radial basis function) is a good default to use for a non-linear kernel** and often is the best non-linear kernel because it usually provides a higher accuracy compared to other non-linear kernels at the cost of higher computational complexity. We can afford to try the RBF kernel because our dataset is small.\n",
        "\n",
        "If you want to explore further please have a look at [this article](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruBbt50ML9hf",
        "outputId": "9f92107e-3784-4ff9-d143-651068f65b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC Model's accuracy on training set is 99.74%\n",
            "SVC Model's accuracy on test set is 52.78%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(\n",
        "    C=10,  #higher the value tighter the margin\n",
        "    gamma='auto',\n",
        "    kernel='rbf',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'SVC Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'SVC Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIm1a-tiL9hg"
      },
      "source": [
        "Not bad at all for the relatively simple SVC model. **Hyperparameter 𝐶 regulates the margin.** It might do well to optimize the SVC model further if we don't find a better one. As it stands, we are looking for considerably higher performance in this task.\n",
        "\n",
        "Check out [this link](https://towardsdatascience.com/visualizing-the-effect-of-hyperparameters-on-support-vector-machines-b9eef6f7357b) for visual representation of affect of changes in C and gamma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j72LJRfCL9hg"
      },
      "source": [
        "### k Nearest Neighbours\n",
        "\n",
        "k Nearest Neighbours (kNN) is next in line, a tried-and-true machine learning method from the 70s. kNN makes a lot of intuitive sense: imagine plotting points on a graph and drawing gates around points that look like they belong to the same group. That's what it is - we **plot our training samples' features and compare a test sample's features' distance to all those points; then just take the _k_ closest points to the test sample and pick the most frequent label/class.** That's it.\n",
        "\n",
        "kNN is a great starting point for multiclass problems with small datasets, although on large dadtasets less reliable and extremely memory hungry (it stores all training sample points). kNN is also useful in that it makes **no assumptions about the underlying distribution of the data set - so kNNs work well for both linear and non-linear data.** In the 2D example:\n",
        "\n",
        "<img src=\"https://github.com/IAT-ExploringAI-2024/Week3-ClassicML/blob/main/images/knn.png?raw=true\" width=400 height=400 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaeCneMiL9hg",
        "outputId": "5c771b63-ba5d-4ef2-93be-eee445e3332d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default kNN Model's accuracy on training set is 66.90%\n",
            "Default kNN Model's accuracy on test set is 46.88%\n",
            "\n",
            "kNN Model's accuracy on training set is 99.74%\n",
            "kNN Model's accuracy on test set is 51.04%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "####### Default kNN  ########\n",
        "model = KNeighborsClassifier(\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'Default kNN Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'Default kNN Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n')\n",
        "\n",
        "##### (hastily) tuned kNN ######\n",
        "model = KNeighborsClassifier(\n",
        "    n_neighbors = 5,\n",
        "    weights = 'distance',\n",
        "    algorithm = 'brute',\n",
        "    n_jobs=4\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'kNN Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'kNN Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVEKUyiL9hh"
      },
      "source": [
        "**The brute-force algorithm computes distances between all pairs of points in the training set; works especially well for small datasets** but wildly inefficient w.r.t. increasing samples and feature space dimension. Not bad for 2 minutes of work, but still not suitable for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQ3av4cL9hh"
      },
      "source": [
        "### Random Forests\n",
        "Finally, and before resorting to deep learning methods, let's try a Random Forest -  a model from the 21st century (2001). **We train many distinct decision trees which are essentially directed acyclic graphs (DAGs), somewhat similar to a flow chart. The collection of (decision) trees makes up our Random Forest.**\n",
        "\n",
        "At each node of the tree we have a function (a rule) that evaluates whether the features of samples input to that node belong to one class or another. Each branch of the tree (or, edge of the graph) defines one of two possible results from a node, and each leaf is one of two decisions made by its parent node. **Each tree in the forest evaluates a random subset of the training samples' features and has a rule at each level of the tree that classifies based on these random features - hence, _Random_ Forest. This random selection of features makes Random Forests robust to outliers**, as such features will have less of an impact in the scope of the entire forest, most of whose trees operate on the \"real\" features.\n",
        "\n",
        "**Random Forests are excellent models to use as a benchmark due to their low time complexity to train and because it is an ensemble method, their robustness to unknown distributions and outliers in the dataset,** meaning Random Forests require relatively little exploratory analysis in both the data and training the model to get an idea of their performance in a task.\n",
        "\n",
        "<img src=\"https://github.com/IliaZenkov/sklearn-audio-classification/blob/master/img/randomforest.png?raw=true\" width=500 height=500 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Random Forest Model's accuracy on training set is 99.83%\n",
            "Default Random Forest Model's accuracy on test set is 60.07%\n",
            "\n",
            "Random Forest Model's accuracy on training set is 99.91%\n",
            "Random Forest Model's accuracy on test set is 61.81%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "####### Default Random Forest ########\n",
        "model = RandomForestClassifier(\n",
        "    random_state=69\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'Default Random Forest Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'Default Random Forest Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n')\n",
        "\n",
        "\n",
        "########## Tuned Random Forest #######\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators = 500,\n",
        "    criterion ='entropy',\n",
        "    warm_start = True,\n",
        "    max_features = 'sqrt',\n",
        "    oob_score = True, # more on this below\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f'Random Forest Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
        "print(f'Random Forest Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9-k8KLL9hi"
      },
      "source": [
        "Not bad for zero effort put into the default model. **Random Forests make a good benchmark model**, especially when strapped for time.\n",
        "\n",
        "**_Max features_ defines size of random feature subset decided upon at each node; sqrt(#features) is a good default for classification.**\n",
        "\n",
        "**_Gini_ and _Entropy_ are functions computing quality of classified samples within each node; they almost always provide similar performance but Entropy is more suited to classification while Gini is better for continuous variables.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnhnxuGXL9hi"
      },
      "source": [
        "\n",
        "As wonderful as Random Forests are, it's clear that we're going to need to pull out bigger guns if we want to get appreciable performance on this dataset, perhaps even with good generalizability on test data. DNNs(Deep Neural Networks) are the next step-up in complexity from classical machine learning models, and we will start at the first rung on that ladder:Simple Perceptron in next lab!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
